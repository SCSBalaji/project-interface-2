"""
GhostConv Module - Efficient convolution using ghost features.

This module implements the Ghost Convolution from the GhostNet paper,
which generates feature maps efficiently by using cheap linear operations
to create "ghost" features from a smaller set of intrinsic features.

Reference:
    Han et al., "GhostNet: More Features from Cheap Operations"
    https://arxiv.org/abs/1911.11907
"""

import math
import torch
import torch.nn as nn
from typing import Optional, Dict, Any

from .utils import BaseBlock


class GhostConv(BaseBlock):
    """
    Ghost Convolution module that generates feature maps efficiently.
    
    Ghost convolution splits the output channels into two parts:
    1. Intrinsic features: Generated by a primary convolution
    2. Ghost features: Generated by cheap depthwise operations on intrinsic features
    
    This approach reduces computational cost while maintaining representational capacity.
    
    Args:
        inp (int): Number of input channels.
        oup (int): Number of output channels.
        kernel_size (int): Kernel size for primary convolution. Default: 1.
        ratio (int): Ratio for splitting channels (intrinsic vs ghost). Default: 2.
        dw_size (int): Kernel size for depthwise (cheap) convolution. Default: 3.
        stride (int): Stride for primary convolution. Default: 1.
        relu (bool): Whether to apply ReLU activation. Default: True.
        debug (bool): Enable debug mode for shape printing. Default: False.
    
    Shape:
        - Input: (B, C_in, H, W)
        - Output: (B, C_out, H', W') where H' = H/stride, W' = W/stride
    
    Examples:
        >>> # Basic usage
        >>> ghost = GhostConv(inp=3, oup=64)
        >>> x = torch.randn(2, 3, 224, 224)
        >>> y = ghost(x)
        >>> y.shape
        torch.Size([2, 64, 224, 224])
        
        >>> # With stride for downsampling
        >>> ghost = GhostConv(inp=64, oup=128, stride=2)
        >>> x = torch.randn(2, 64, 56, 56)
        >>> y = ghost(x)
        >>> y.shape
        torch.Size([2, 128, 28, 28])
    
    Note:
        The actual number of channels before slicing is:
        init_channels + new_channels = ceil(oup/ratio) + ceil(oup/ratio) * (ratio-1)
        which may exceed 'oup', hence the final slicing operation.
        
        When ratio=1, no ghost features are generated (standard convolution).
    """
    
    def __init__(
        self,
        inp: int,
        oup: int,
        kernel_size: int = 1,
        ratio: int = 2,
        dw_size: int = 3,
        stride: int = 1,
        relu: bool = True,
        debug: bool = False
    ):
        super().__init__(debug=debug)
        
        # Validate parameters
        assert inp > 0, f"Input channels must be positive, got {inp}"
        assert oup > 0, f"Output channels must be positive, got {oup}"
        assert kernel_size > 0 and kernel_size % 2 == 1, \
            f"Kernel size must be positive and odd, got {kernel_size}"
        assert ratio >= 1, f"Ratio must be >= 1, got {ratio}"
        assert dw_size > 0 and dw_size % 2 == 1, \
            f"Depthwise kernel size must be positive and odd, got {dw_size}"
        assert stride >= 1, f"Stride must be >= 1, got {stride}"
        
        # Store configuration
        self.inp = inp
        self.oup = oup
        self.kernel_size = kernel_size
        self.ratio = ratio
        self.dw_size = dw_size
        self.stride = stride
        self.use_relu = relu
        
        # Calculate channel split
        # When ratio=1, all channels are intrinsic (no ghost features)
        if ratio == 1:
            self.init_channels = oup
            self.new_channels = 0
        else:
            self.init_channels = math.ceil(oup / ratio)
            self.new_channels = self.init_channels * (ratio - 1)
        
        # Primary convolution: generates intrinsic features
        self.primary_conv = nn.Sequential(
            nn.Conv2d(
                inp, 
                self.init_channels, 
                kernel_size=kernel_size, 
                stride=stride, 
                padding=kernel_size // 2, 
                bias=False
            ),
            nn.BatchNorm2d(self.init_channels),
            nn.ReLU(inplace=True) if relu else nn.Identity()
        )
        
        # Cheap operation: generates ghost features via depthwise conv
        # Only create if we have ghost features to generate
        if self.new_channels > 0:
            self.cheap_op = nn.Sequential(
                nn.Conv2d(
                    self.init_channels, 
                    self.new_channels, 
                    kernel_size=dw_size, 
                    stride=1, 
                    padding=dw_size // 2,
                    groups=self.init_channels,  # Depthwise convolution
                    bias=False
                ),
                nn.BatchNorm2d(self.new_channels),
                nn.ReLU(inplace=True) if relu else nn.Identity()
            )
        else:
            self.cheap_op = None
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass of GhostConv.
        
        Args:
            x: Input tensor of shape (B, C_in, H, W)
        
        Returns:
            Output tensor of shape (B, C_out, H', W')
        """
        self._debug_print("Input", x)
        
        # Generate intrinsic features
        x1 = self.primary_conv(x)
        self._debug_print("After primary conv (intrinsic)", x1)
        
        # If ratio=1, no ghost features - just return intrinsic features
        if self.cheap_op is None:
            out = x1[:, :self.oup, :, :]
            self._debug_print("Output (no ghost features)", out)
            return out
        
        # Generate ghost features
        x2 = self.cheap_op(x1)
        self._debug_print("After cheap op (ghost)", x2)
        
        # Concatenate intrinsic and ghost features
        out = torch.cat([x1, x2], dim=1)
        self._debug_print("After concatenation", out)
        
        # Slice to exact output channels
        out = out[:, :self.oup, :, :]
        self._debug_print("After slicing to oup", out)
        
        return out
    
    def extra_repr(self) -> str:
        """Return string representation with configuration."""
        return (
            f"inp={self.inp}, oup={self.oup}, "
            f"kernel_size={self.kernel_size}, ratio={self.ratio}, "
            f"dw_size={self.dw_size}, stride={self.stride}, relu={self.use_relu}"
        )
    
    def get_config(self) -> Dict[str, Any]:
        """Return configuration dictionary."""
        return {
            'class': self.__class__.__name__,
            'inp': self.inp,
            'oup': self.oup,
            'kernel_size': self.kernel_size,
            'ratio': self.ratio,
            'dw_size': self.dw_size,
            'stride': self.stride,
            'relu': self.use_relu,
            'init_channels': self.init_channels,
            'new_channels': self.new_channels,
        }